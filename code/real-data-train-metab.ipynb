{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor, TPOTClassifier\n",
    "from tpot.export_utils import generate_import_code, generate_export_pipeline_code\n",
    "from tpot.export_utils import export_pipeline, expr_to_tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from tpot.config.classifier_nn import classifier_config_nn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from tpot.config import regressor_config_dict_light\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.datasets import load_digits\n",
    "from utils import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import glob\n",
    "import itertools\n",
    "from time import process_time\n",
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables\n",
    "n_gen = 50\n",
    "n_pop = 50\n",
    "\n",
    "my_datasets = ['METAB']\n",
    "\n",
    "make_func = 'def opt_pipe(training_features, testing_features):\\n'\n",
    "import_impute = 'from sklearn.impute import SimpleImputer\\n\\n'\n",
    "impute_text = '\\timputer = SimpleImputer(strategy=\"median\")\\n\\timputer.fit(training_features)\\n\\t\\\n",
    "training_features = imputer.transform(training_features)\\n\\t\\\n",
    "testing_features = imputer.transform(testing_features)\\n'\n",
    "\n",
    "def write_pipes(name, tpot):\n",
    "    \"\"\"Write TPOT pipelines out to subdirectories.\"\"\"\n",
    "    import_codes = generate_import_code(tpot._optimized_pipeline, tpot.operators)\n",
    "    pipeline_codes = generate_export_pipeline_code(expr_to_tree(tpot._optimized_pipeline,tpot._pset), tpot.operators)\n",
    "    pipe_text = import_codes.replace('import numpy as np\\nimport pandas as pd\\n', 'from sklearn.preprocessing import FunctionTransformer\\nfrom copy import copy\\n')\n",
    "    if tpot._imputed: # add impute code when there is missing data\n",
    "        pipe_text += import_impute + make_func + impute_text\n",
    "    else:\n",
    "        pipe_text += make_func\n",
    "    pipe_text += '\\texported_pipeline = ' + pipeline_codes + \"\\n\\treturn({'train_feat': training_features, 'test_feat': testing_features, 'pipe': exported_pipeline})\"\n",
    "    f = open(name + '.py', 'w')\n",
    "    f.write(pipe_text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_config = regressor_config_dict_light\n",
    "\n",
    "personal_config['sklearn.neural_network.MLPRegressor'] = {\n",
    "    # MLPClassifier for neural networks\n",
    "    # TODO: revisit/tweak: alpha, momentum, learning rate_init\n",
    "    # separater paras based on activation\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'hidden_layer_sizes': [(256, 64, )],\n",
    "    'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n",
    "    'learning_rate_init': [1e-3, 1e-2, 1e-1, 0.5, 0.75, 0.9],\n",
    "    'momentum': [0.1, 0.5, 0.75, 0.9]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scoring_fn = 'r2'\n",
    "# def my_r2(y, y_hat):\n",
    "#     '''Calculates the R^2 for set of observations y and the predictions y_hat. \n",
    "#     Same metric in DNN paper: squared Pearson correlation coefficient \n",
    "#     between predicted and observed activities in the test set.\n",
    "#     Assumes these two are np arrays.\n",
    "#     '''\n",
    "#     r2 = np.corrcoef(y_hat, y)[0,1]**2\n",
    "#     return(r2)\n",
    "# scoring_fn = make_scorer(my_r2, greater_is_better = True)\n",
    "\n",
    "random_state = 1618\n",
    "path = ''\n",
    "extension = 'csv'\n",
    "data_dir = 'qsar/'\n",
    "label = 'Act'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tpot_structure(outcome = 'quant'): \n",
    "    if outcome == 'binary':\n",
    "        tpot = TPOTClassifier(generations = n_gen, \n",
    "                         population_size = n_pop, \n",
    "                         verbosity = 2,\n",
    "                         config_dict = personal_config,\n",
    "                         scoring = scoring_fn,\n",
    "                         random_state = random_state,\n",
    "                         cv = TimeSeriesSplit(n_splits=5),\n",
    "                         template = 'Selector-Transformer-MLPClassifier')\n",
    "    else: # quantitative trait\n",
    "        tpot = TPOTRegressor(generations = n_gen, \n",
    "                         population_size = n_pop, \n",
    "                         verbosity = 2,\n",
    "                         config_dict = personal_config,\n",
    "                         scoring = scoring_fn,\n",
    "                         random_state = random_state,\n",
    "                         cv = TimeSeriesSplit(n_splits=5),\n",
    "                         template = 'Selector-Transformer-MLPRegressor')\n",
    "    return tpot\n",
    "\n",
    "def run_tpot(dat_name, outcome = 'quant'):\n",
    "    tpot = build_tpot_structure(outcome)\n",
    "    \n",
    "    # Read in the data:\n",
    "    train_data = pd.read_csv(data_dir + dat_name + '_training_preprocessed.csv', index_col = 'MOLECULE')\n",
    "    X_train = train_data.drop(label, axis=1)\n",
    "    y_train = train_data[label]\n",
    "    del train_data\n",
    "    \n",
    "    ### Run TPOT with `template`:\n",
    "    t_start = process_time() # start timing\n",
    "    tpot.fit(X_train.values, y_train)\n",
    "    t_stop = process_time() # end timing\n",
    "    \n",
    "    print('Total elapsed process time:', t_stop - t_start, 'seconds')\n",
    "    write_pipes('pipelines/' + dat_name + '_' + str(random_state), tpot)\n",
    "    CV_score = tpot._optimized_pipeline_score\n",
    "    delta_t = t_stop - t_start\n",
    "\n",
    "    return {'CV_R2_score': CV_score, 'Elapsed time': delta_t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtypes = {'datasets': my_datasets,\n",
    "          'outcomes':['quant']}\n",
    "mtype_grid = expand_grid(mtypes) # data type grid\n",
    "\n",
    "results = mtype_grid.apply(\n",
    "    lambda r: run_tpot(r.datasets, r.outcomes), \n",
    "    axis = 1, result_type = 'expand')\n",
    "results\n",
    "final_results = pd.concat([mtype_grid, results], axis = 1)\n",
    "final_results.to_csv('accuracies/qsar_' + str(random_state) + \"_\" + str(n_gen) + \"_\" + str(n_pop) + \".csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
